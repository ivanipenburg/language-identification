{"cells":[{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 117500\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 117500\n","    })\n","})\n"]}],"source":["\n","from src.data import get_dataloaders\n","from src.data import load_wili_dataset\n","import os\n","\n","datasets = load_wili_dataset('data')\n","print(datasets)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Filter: 100%|██████████| 117500/117500 [00:00<00:00, 261480.42 examples/s]\n","Filter: 100%|██████████| 117500/117500 [00:00<00:00, 279155.72 examples/s]\n"]}],"source":["# Test on only 4 languages\n","languages = ['eng', 'deu', 'fra', 'nld']\n","for split in datasets:\n","    datasets[split] = datasets[split].filter(lambda example: example['label'] in languages)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 2000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 2000\n","    })\n","})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["datasets"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.pre_tokenizers import Whitespace\n","from tokenizers.trainers import BpeTrainer\n","\n","DATA_DIR = 'data'\n","\n","TOKENIZER_FILE = 'src/BPE_trained.json'\n","\n","def train_BPE(languages):\n","\n","    # create a .txt file that only contains the specific languages on which we want to train the BPE tokenizer\n","    output_path = os.path.join(DATA_DIR, 'BPE_train.txt')\n","    with open(output_path, 'w', encoding='utf-8') as output_file:\n","        with open(os.path.join(DATA_DIR,'x_train.txt' ), 'r', encoding='utf-8') as x_file, open(os.path.join(DATA_DIR,'y_train.txt' ), 'r', encoding='utf-8') as y_file:\n","            for x1, y1 in zip(x_file, y_file):\n","                x1 = x1.strip()\n","                y1 = y1.strip()\n","                if y1 in languages:\n","                    output_file.write(x1 + '\\n')\n","\n","    unk_token = \"<UNK>\"  # token for unknown words\n","    spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]\n","    file = [output_path]\n","\n","    tokenizer = Tokenizer(BPE(unk_token = unk_token))\n","    tokenizer.pre_tokenizer = Whitespace()\n","    trainer = BpeTrainer(special_tokens = spl_tokens)\n","    tokenizer.train(file, trainer)\n","    tokenizer.save(TOKENIZER_FILE)\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["tokenizer = train_BPE(['eng'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"dl4nlp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
